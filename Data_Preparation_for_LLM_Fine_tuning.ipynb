{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0984654db0e74e5bb937f55c318c96aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a6c560b9dc04ccb8683286054bf2ec2",
              "IPY_MODEL_05cfaca10dc84d86b1ef59d56c319868",
              "IPY_MODEL_0438903fb59e4ef896cde478130077de"
            ],
            "layout": "IPY_MODEL_ab1876299e8548b2a64cf57fb0bb1193"
          }
        },
        "9a6c560b9dc04ccb8683286054bf2ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58f00f421054fb8beefd059286a78d7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4729334718714b02b28dad10f55a1419",
            "value": "Map:â€‡100%"
          }
        },
        "05cfaca10dc84d86b1ef59d56c319868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cf86c335c50487fb183ad6c018366f1",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c6c2a4ebef64294aff432fd3148ab90",
            "value": 5
          }
        },
        "0438903fb59e4ef896cde478130077de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e761fd246dfe4a1d8ff88d05033ab262",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_872f25972070488c9714215212fb4d41",
            "value": "â€‡5/5â€‡[00:00&lt;00:00,â€‡40.24â€‡examples/s]"
          }
        },
        "ab1876299e8548b2a64cf57fb0bb1193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d58f00f421054fb8beefd059286a78d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4729334718714b02b28dad10f55a1419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cf86c335c50487fb183ad6c018366f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c6c2a4ebef64294aff432fd3148ab90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e761fd246dfe4a1d8ff88d05033ab262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "872f25972070488c9714215212fb4d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dk-agathya/Profile/blob/main/Data_Preparation_for_LLM_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X5xI2GQ-ick"
      },
      "outputs": [],
      "source": [
        "# Data Preparation for LLM Fine-tuning\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"ðŸš€ Data Preparation Tutorial (Following Original Notebook)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "print(\"Step 1: Import necessary libraries\")\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQmiBKcM-jV6",
        "outputId": "0263fa12-7d3d-4742-afab-da43bb2a9feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Data Preparation Tutorial (Following Original Notebook)\n",
            "============================================================\n",
            "Step 1: Import necessary libraries\n",
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load tokenizer\n",
        "print(\"\\nStep 2: Load tokenizer\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"âœ… Tokenizer loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7XV77hA5Mt",
        "outputId": "f517627f-1554-4802-8873-f8a832290bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 2: Load tokenizer\n",
            "âœ… Tokenizer loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and prepare the dataset\n",
        "print(\"\\nStep 3: Load and prepare the dataset\")\n",
        "\n",
        "# Sample data\n",
        "sample_data = [\n",
        "    {\n",
        "        \"question\": \"What are the different types of documents available in the repository?\",\n",
        "        \"answer\": \"Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the recommended way to set up and configure the code repository?\",\n",
        "        \"answer\": \"Lamini can be downloaded as a python package and used in any codebase that uses python. Additionally, we provide a language agnostic REST API.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can I find the specific documentation I need for a particular feature or function?\",\n",
        "        \"answer\": \"You can ask this model about documentation, which is trained on our publicly available docs and source code, or you can go to https://lamini-ai.github.io/.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Does the documentation include explanations of the code's purpose?\",\n",
        "        \"answer\": \"Our documentation provides both real-world and toy examples of how one might use Lamini in a larger system.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Does the documentation provide information about external dependencies?\",\n",
        "        \"answer\": \"External dependencies and libraries are all available on the Python package hosting website Pypi at https://pypi.org/project/lamini/\"\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "5PnpilQoCU2j",
        "outputId": "12e05754-a0cf-49bc-e11a-e1b649e2f4a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Load and prepare the dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "instruction_dataset = pd.DataFrame(sample_data)\n",
        "print(f\"âœ… Dataset loaded with {len(instruction_dataset)} examples\")\n",
        "\n",
        "# Convert to dictionary format\n",
        "examples = instruction_dataset.to_dict()\n",
        "\n",
        "# Extract text data\n",
        "if \"question\" in examples and \"answer\" in examples:\n",
        "    text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
        "elif \"instruction\" in examples and \"response\" in examples:\n",
        "    text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
        "elif \"input\" in examples and \"output\" in examples:\n",
        "    text = examples[\"input\"][0] + examples[\"output\"][0]\n",
        "else:\n",
        "    text = examples[\"text\"][0]\n",
        "\n",
        "print(\"Sample text extracted:\", text[:100] + \"...\")"
      ],
      "metadata": {
        "id": "UQ8GMMf7CXde",
        "outputId": "705309a1-62ac-4912-90a7-900f5c1df4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset loaded with 5 examples\n",
            "Sample text extracted: What are the different types of documents available in the repository?Lamini has documentation on Ge...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 4: Format data for fine-tuning\n",
        "print(\"\\nStep 4: Format data for fine-tuning\")\n"
      ],
      "metadata": {
        "id": "PF48zD1CCbSQ",
        "outputId": "cc7ac1c6-367d-4bfd-f670-5e85edd8bc4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 4: Format data for fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "prompt_template = \"\"\"### Question:\n",
        "{question}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "num_examples = len(examples[\"question\"])\n",
        "finetuning_data = []\n",
        "\n",
        "for i in range(num_examples):\n",
        "    question = examples[\"question\"][i]\n",
        "    answer = examples[\"answer\"][i]\n",
        "    text_with_prompt_template = prompt_template.format(question=question)\n",
        "    finetuning_data.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
        "\n",
        "print(\"âœ… Data formatted!\")\n",
        "print(\"Sample datapoint:\")\n",
        "pprint(finetuning_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-XqzB5RN1cA",
        "outputId": "3fc503c7-2324-4226-bd5b-40e191c13a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data formatted!\n",
            "Sample datapoint:\n",
            "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
            "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
            "           'Advanced topics, and class documentation on LLM Engine available '\n",
            "           'at https://lamini-ai.github.io/.',\n",
            " 'question': '### Question:\\n'\n",
            "             'What are the different types of documents available in the '\n",
            "             'repository?\\n'\n",
            "             '\\n'\n",
            "             '### Answer:'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Tokenize a single example\n",
        "print(\"\\nStep 5: Tokenize a single example\")\n",
        "\n",
        "\n",
        "text = finetuning_data[0][\"question\"] + finetuning_data[0][\"answer\"]\n",
        "\n",
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=True\n",
        ")\n",
        "print(\"Token IDs:\", tokenized_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR-5JRVIOAUm",
        "outputId": "f452c4b5-186b-4423-ba45-995a647984bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5: Tokenize a single example\n",
            "Token IDs: [[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n",
            "    275   253 18491    32   187   187  4118 37741    27    45  4988    74\n",
            "    556 10097   327 27669 11075   264    13  5271 23058    13 19782 37741\n",
            "  10031    13 13814 11397    13   378 16464    13 11759 10535  1981    13\n",
            "  21798 12989    13   285   966 10097   327 21708    46 10797  2130   387\n",
            "   5987  1358    77  4988    74    14  2284    15  7280    15   900 14206]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Handle long sequences\n",
        "print(\"\\nStep 6: Handle long sequences\")\n",
        "\n",
        "\n",
        "max_length = 2048\n",
        "max_length = min(\n",
        "    tokenized_inputs[\"input_ids\"].shape[1],\n",
        "    max_length,\n",
        ")\n",
        "\n",
        "print(f\"Using max_length: {max_length}\")\n",
        "\n",
        "# Apply truncation\n",
        "tokenized_inputs = tokenizer(\n",
        "    text,\n",
        "    return_tensors=\"np\",\n",
        "    truncation=True,\n",
        "    max_length=max_length\n",
        ")\n",
        "print(\"âœ… Truncation applied\")\n",
        "print(\"Final shape:\", tokenized_inputs[\"input_ids\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq2LyVtwOCpS",
        "outputId": "7a828246-c0da-49cf-f2a5-0e38729b3a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Handle long sequences\n",
            "Using max_length: 72\n",
            "âœ… Truncation applied\n",
            "Final shape: (1, 72)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Create tokenization function\n",
        "print(\"\\nStep 7: Create tokenization function\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize function following original notebook logic\"\"\"\n",
        "\n",
        "    # Handle the batched format\n",
        "    if \"question\" in examples and \"answer\" in examples:\n",
        "        # Combine question and answer for each example in batch\n",
        "        texts = []\n",
        "        for i in range(len(examples[\"question\"])):\n",
        "            text = examples[\"question\"][i] + examples[\"answer\"][i]\n",
        "            texts.append(text)\n",
        "    else:\n",
        "        texts = examples[\"text\"]\n",
        "\n",
        "    # Set pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Get initial tokenization\n",
        "    tokenized_inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=None,\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    # Handle max length\n",
        "    if len(texts) > 0:\n",
        "        # Get the longest sequence in this batch\n",
        "        first_tokenized = tokenizer(texts[0], return_tensors=\"np\", padding=True)\n",
        "        current_max_length = min(first_tokenized[\"input_ids\"].shape[1], 2048)\n",
        "    else:\n",
        "        current_max_length = 2048\n",
        "\n",
        "    # Set truncation side\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "\n",
        "    # Final tokenization with truncation\n",
        "    tokenized_inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=None,\n",
        "        truncation=True,\n",
        "        max_length=current_max_length\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "print(\"âœ… Tokenization function created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuwH3ck7OFcN",
        "outputId": "a8436021-d0df-43d9-e050-53cfe0282074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 7: Create tokenization function\n",
            "âœ… Tokenization function created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Tokenize the entire dataset (using HuggingFace )\n",
        "print(\"\\nStep 8: Tokenize the entire dataset\")\n",
        "\n",
        "# Create dataset from our formatted data\n",
        "dataset = Dataset.from_list(finetuning_data)\n",
        "\n",
        "# Apply tokenization with same parameters\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,\n",
        "    drop_last_batch=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Dataset tokenized!\")\n",
        "print(f\"Tokenized dataset: {tokenized_dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "0984654db0e74e5bb937f55c318c96aa",
            "9a6c560b9dc04ccb8683286054bf2ec2",
            "05cfaca10dc84d86b1ef59d56c319868",
            "0438903fb59e4ef896cde478130077de",
            "ab1876299e8548b2a64cf57fb0bb1193",
            "d58f00f421054fb8beefd059286a78d7",
            "4729334718714b02b28dad10f55a1419",
            "3cf86c335c50487fb183ad6c018366f1",
            "2c6c2a4ebef64294aff432fd3148ab90",
            "e761fd246dfe4a1d8ff88d05033ab262",
            "872f25972070488c9714215212fb4d41"
          ]
        },
        "id": "bbtwBwC3OH84",
        "outputId": "d6cbe1a7-399a-415f-d7f4-93ca54510e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 8: Tokenize the entire dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0984654db0e74e5bb937f55c318c96aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset tokenized!\n",
            "Tokenized dataset: Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Add labels\n",
        "print(\"\\nStep 9: Add labels\")\n",
        "\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
        "\n",
        "print(\"âœ… Labels added!\")\n",
        "print(f\"Dataset with labels: {tokenized_dataset}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knfye01eOKN7",
        "outputId": "9b7c5419-c962-41ad-edc1-08f0a4befa1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 9: Add labels\n",
            "âœ… Labels added!\n",
            "Dataset with labels: Dataset({\n",
            "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 5\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Create train/test splits\n",
        "print(\"\\nStep 10: Create train/test splits\")\n",
        "\n",
        "\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "\n",
        "print(\"âœ… Dataset split complete!\")\n",
        "print(f\"Training examples: {len(split_dataset['train'])}\")\n",
        "print(f\"Test examples: {len(split_dataset['test'])}\")\n",
        "\n",
        "# Show final structure\n",
        "print(f\"\\nFinal dataset structure:\")\n",
        "print(split_dataset)\n",
        "\n",
        "# Convert to pandas\n",
        "train_df = pd.DataFrame(split_dataset[\"train\"])\n",
        "test_df = pd.DataFrame(split_dataset[\"test\"])\n",
        "\n",
        "print(f\"\\nTrain DataFrame shape: {train_df.shape}\")\n",
        "print(f\"Test DataFrame shape: {test_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fy426vgOPd3",
        "outputId": "d976bed1-5c5f-4a26-db6c-bc47f516f7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 10: Create train/test splits\n",
            "âœ… Dataset split complete!\n",
            "Training examples: 4\n",
            "Test examples: 1\n",
            "\n",
            "Final dataset structure:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 4\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "})\n",
            "\n",
            "Train DataFrame shape: (4, 5)\n",
            "Test DataFrame shape: (1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show sample data\n",
        "print(f\"\\nSample from training set:\")\n",
        "# Get individual examples properly\n",
        "for i in range(min(2, len(split_dataset[\"train\"]))):  # Show up to 2 examples\n",
        "    example = split_dataset[\"train\"][i]\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  Question: {example['question'][:100]}...\")\n",
        "    print(f\"  Answer: {example['answer'][:100]}...\")\n",
        "    print(f\"  Input IDs length: {len(example['input_ids'])}\")\n",
        "    print(f\"  Labels length: {len(example['labels'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYo4RwtJOS5Z",
        "outputId": "3dee2608-68e7-4252-fce5-55131098aace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample from training set:\n",
            "Example 1:\n",
            "  Question: ### Question:\n",
            "What are the different types of documents available in the repository?\n",
            "\n",
            "### Answer:...\n",
            "  Answer: Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, ...\n",
            "  Input IDs length: 72\n",
            "  Labels length: 72\n",
            "Example 2:\n",
            "  Question: ### Question:\n",
            "How can I find the specific documentation I need for a particular feature or function?...\n",
            "  Answer: You can ask this model about documentation, which is trained on our publicly available docs and sour...\n",
            "  Input IDs length: 62\n",
            "  Labels length: 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final statistics\n",
        "print(f\"\\nðŸ“Š Final Statistics:\")\n",
        "train_lengths = [len(example['input_ids']) for example in split_dataset['train']]\n",
        "test_lengths = [len(example['input_ids']) for example in split_dataset['test']]\n",
        "\n",
        "import numpy as np\n",
        "print(f\"Training set:\")\n",
        "print(f\"  Average length: {np.mean(train_lengths):.1f} tokens\")\n",
        "print(f\"  Min length: {min(train_lengths)} tokens\")\n",
        "print(f\"  Max length: {max(train_lengths)} tokens\")\n",
        "\n",
        "print(f\"Test set:\")\n",
        "print(f\"  Average length: {np.mean(test_lengths):.1f} tokens\")\n",
        "print(f\"  Min length: {min(test_lengths)} tokens\")\n",
        "print(f\"  Max length: {max(test_lengths)} tokens\")\n",
        "\n",
        "# Conclusion\n",
        "print(f\"\\nðŸŽ‰ DATA PREPARATION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "âœ… WHAT WE ACCOMPLISHED:\n",
        "1. Loaded and prepared the dataset\n",
        "2. Formatted data with prompt template\n",
        "3. Tokenized single example for testing\n",
        "4. Handled long sequences with truncation\n",
        "5. Created tokenization function\n",
        "6. Tokenized entire dataset\n",
        "7. Added labels for training\n",
        "8. Created train/test splits\n",
        "9. Analyzed final dataset\n",
        "\n",
        "ðŸš€ NEXT STEPS:\n",
        "This concludes the data preparation process for fine-tuning a Language Learning Model.\n",
        "The next steps would involve setting up the model, fine-tuning it on the training data,\n",
        "and evaluating its performance on the test data.\n",
        "\n",
        "Your split_dataset is ready for training!\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAMAFMP6BtVE",
        "outputId": "15f8bc1b-5918-458a-a015-aecdf9efec7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Final Statistics:\n",
            "Training set:\n",
            "  Average length: 58.0 tokens\n",
            "  Min length: 43 tokens\n",
            "  Max length: 72 tokens\n",
            "Test set:\n",
            "  Average length: 47.0 tokens\n",
            "  Min length: 47 tokens\n",
            "  Max length: 47 tokens\n",
            "\n",
            "ðŸŽ‰ DATA PREPARATION COMPLETE!\n",
            "============================================================\n",
            "\n",
            "âœ… WHAT WE ACCOMPLISHED:\n",
            "1. Loaded and prepared the dataset\n",
            "2. Formatted data with prompt template\n",
            "3. Tokenized single example for testing\n",
            "4. Handled long sequences with truncation\n",
            "5. Created tokenization function\n",
            "6. Tokenized entire dataset\n",
            "7. Added labels for training\n",
            "8. Created train/test splits\n",
            "9. Analyzed final dataset\n",
            "\n",
            "ðŸš€ NEXT STEPS:\n",
            "This concludes the data preparation process for fine-tuning a Language Learning Model.\n",
            "The next steps would involve setting up the model, fine-tuning it on the training data,\n",
            "and evaluating its performance on the test data.\n",
            "\n",
            "Your split_dataset is ready for training!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwK13I0kB43v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}